version: '2.1'

services:
  ### Michael Jahns
  weatherinput:
    image: laus.fzi.de:8201/weatherba/weather_input
    depends_on:
      - "consul"
      - "kafka"
    ports:
      - "8087:8087"
#    volumes:
#      - ./templates-sources:/root/.ba/weather
    networks:
      spnet:

  restproducer:
    image: laus.fzi.de:8201/weatherba/restproducer
    depends_on:
      - "consul"
    ports:
      - "8092:8085"
    volumes:
      - ./data/restproducer:/src/main/java/ba/restinterface
    networks:
      spnet:

  dextractor:
    image: laus.fzi.de:8201/weatherba/dextractor
    depends_on:
      - "consul"
      - "jobmanager"
      - "kafka"
    ports:
      - "8086:8086"
 #   volumes:
 #     - ./weather:/root/.ba/dextractor
    networks:
      spnet:


  ### Jochen Lutz
  sensebox-simple-rest-server:
    image: biggis/sbsrs:0.2.3
    hostname: sbsrs
    depends_on:
      - kafka
    ports:
      - "9100:9100"
    environment:
      KAFKA_SERVER: kafka:9092
    networks:
      spnet:

  ssh-tunnel:
    image: mikenowak/ssh
    command: "sshtunnel"
    ports:
      - "8022:22"
      #- "8085:8085"
    volumes:
      - ./data/ssh-tunnel/hostkeys:/etc/ssh/hostkeys
      - ./data/ssh-tunnel/home:/home
    networks:
      spnet:

  codekunst-mqtt-adapter:
    image: biggis/codekunst-mqtt:3.0.0
    restart: always
    depends_on:
      - kafka
    environment:
      KAFKA_SERVER: kafka:9092
      MQTT_USER: "fzi-biggis-sensors"
      MQTT_PASSWORD: "ttn-account-v2.rYK7Ak_JKS2pfk8TNmuMZe3FRetPGvMAZmx09S2Bm98"
      CODEKUNST_SERVER: "ssl://eu.thethings.network:8883"
      CODEKUNST_TOPIC: "+/devices/+/up"
    networks:
      spnet:

  sensebox-adapter:
    image: laus.fzi.de:8201/biggisstreampipes/senseboxadapter
    depends_on:
      - "consul"
    networks:
      spnet:

  sensebox-metadata-enricher:
    image: laus.fzi.de:8201/biggisstreampipes/senseboxmetadataenricher
    depends_on:
      - "consul"
    networks:
      spnet:

  rasterdata-adapter:
    image: laus.fzi.de:8201/biggisstreampipes/rasterdata-adapter
    depends_on:
      - "consul"
    networks:
      spnet:

  rasterdata-endless-source:
    image: laus.fzi.de:8201/biggisstreampipes/rasterdata-endless-source
    depends_on:
      - "kafka"
    environment:
      JAVA_OPTS: "-Dkafka.server=kafka:9092"
    volumes:
      - "./data/tiles-tp:/tiles"
    networks:
      spnet:


  ### StreamPipes
  backend:
    image: laus.fzi.de:8201/streampipes/ce-backend
    depends_on:
      - "kafka"
      - "consul"
    ports:
      - "8030:8030"
    volumes:
      - "./config:/root/.streampipes"
      - "./config/aduna:/root/.aduna"
    networks:
      spnet:

  pe-sources-samples:
    image: laus.fzi.de:8201/streampipes/pe-sources-samples
    depends_on:
      - "consul"
#    ports:
#      - "8089:8089"
    volumes:
      - "./config:/root/.streampipes"
    networks:
      spnet:

  pe-esper:
    image: laus.fzi.de:8201/streampipes/pe-processors-esper
    depends_on:
      - "consul"
#    ports:
#      - "8090:8090"
    volumes:
      - "./config:/root/.streampipes"
    networks:
      spnet:

  pe-sinks:
    image: laus.fzi.de:8201/streampipes/pe-sinks-standalone
    depends_on:
      - "consul"
    environment:
      - STREAMPIPES_HOST=${HOSTNAME:-localhost}
#    ports:
#      - "8091:8091"
    volumes:
      - "./config:/root/.streampipes"
    networks:
      spnet:

  pe-flink-samples:
    image: laus.fzi.de:8201/streampipes/pe-mixed-flink
    depends_on:
      - "consul"
#    ports:
#      - "8094:8094"
    volumes:
      - "./config:/root/.streampipes"
    networks:
      spnet:


  ### externe Komponenten
  consul:
    image: consul
    environment:
      - "CONSUL_LOCAL_CONFIG={\"disable_update_check\": true}"
      - "CONSUL_BIND_INTERFACE=eth0"
      - "CONSUL_HTTP_ADDR=0.0.0.0"
    entrypoint:
      - consul
      - agent
      - -server
      - -bootstrap-expect=1
      - -data-dir=/consul/data
      - -node=consul-one
      - -bind={{ GetInterfaceIP "eth0" }}
      - -client=0.0.0.0
      - -enable-script-checks=true
      - -ui
    volumes:
      - ./config/consul:/consul/data
    ports:
      - "8500:8500"
      - "8600:8600"
    networks:
      spnet:
        ipv4_address: 172.30.0.9

  zookeeper:
    image: wurstmeister/zookeeper
    ports:
      - "2181:2181"
    #volumes:
    #  - ./config/zookeeper-data:/tmp/zookeeper/data
    networks:
      spnet:

  kafka:
    image: wurstmeister/kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_ADVERTISED_HOST_NAME: kafka
      KAFKA_ADVERTISED_PORT: 9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    volumes:
#      - ./config/kafka-data:/tmp/kafka/data
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      spnet:

  jobmanager:
    image: laus.fzi.de:8201/streampipes/flink
    ports:
      - "8099:8099"
    command: jobmanager
    networks:
      spnet:

  taskmanager:
    image: laus.fzi.de:8201/streampipes/flink
    depends_on:
      - jobmanager
    environment:
      - FLINK_NUM_SLOTS=20
    command: taskmanager
    networks:
      spnet:

  kafka-web-console:
    image: hwestphal/kafka-web-console
    ports:
      - "9000:9000"
    volumes:
      - "./config/kafka-web-console:/data hwestphal/kafka-web-console"
    networks:
      spnet:

  nginx:
    image: laus.fzi.de:8201/streampipes/nginx
    depends_on:
      - backend
    ports:
      - "80:80"
    volumes:
      - "./data/nginx-static-data:/usr/share/nginx/html/static-data"
    networks:
      spnet:

  couchdb:
    image: couchdb
    ports:
      - "5984:5984"
    volumes:
      - "./config/couchdb/data:/usr/local/var/lib/couchdb"
    networks:
      spnet:

  activemq:
    image: laus.fzi.de:8201/streampipes/activemq
    ports:
      - "61616:61616"
      - "61614:61614"
      - "8161:8161"
    networks:
      spnet:

#  elasticsearch:
#    image: elasticsearch:5.2.2-alpine
#    ports:
#      - "9200:9200"
#      - "9300:9300"
#    volumes:
#      - ./config/elasticsearch/data:/usr/share/elasticsearch/data
#      - ./config/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
#    networks:
#      spnet:
#
#  kibana:
#    image: kibana:5.2.2
#    ports:
#      - "5601:5601"
#    depends_on:
#      - elasticsearch
#    volumes:
#      - ./config/kibana/kibana.yml:/opt/kibana/config/kibana.yml
#    networks:
#      spnet:

  hdfs-name:
    image: biggis/hdfs:2.7.1
    ports:
      - "50070:50070"
      - "8020:8020"
    command: start.sh name
    environment:
      USER_ID: ${USER_ID-1000}
      USER_NAME: hdfs
      HADOOP_MASTER_ADDRESS: hdfs-name
      TIMEZONE: Europe/Berlin
    volumes:
      - ./data/hdfs-data:/data/hdfs
    networks:
      spnet:

  hdfs-sname:
    image: biggis/hdfs:2.7.1
    depends_on:
        - hdfs-name
    ports:
      - "50090:50090"
    command: start.sh sname
    environment:
      USER_ID: ${USER_ID-1000}
      USER_NAME: hdfs
      HADOOP_MASTER_ADDRESS: hdfs-name
      TIMEZONE: Europe/Berlin
    volumes:
      - ./data/hdfs-data:/data/hdfs
    networks:
      spnet:

  hdfs-data:
    image: biggis/hdfs:2.7.1
    depends_on:
        - hdfs-name
    ports:
      - "50010:50010"
      - "50075:50075"
    command: start.sh data
    environment:
      USER_ID: ${USER_ID-1000}
      USER_NAME: hdfs
      HADOOP_MASTER_ADDRESS: hdfs-name
      TIMEZONE: Europe/Berlin
    volumes:
      - ./data/hdfs-data:/data/hdfs
    networks:
      spnet:

  hdfs-client:
    image: biggis/hdfs-client:2.7.1
    command: upload.sh -copyFromLocal /data/hdfs/hamlet.txt /
    environment:
      USER_ID: ${USER_ID-1000}
      USER_NAME: hdfs
      HADOOP_MASTER_ADDRESS: hdfs-name
      TIMEZONE: Europe/Berlin
    volumes:
      - ./data/hamlet.txt:/data/hdfs/hamlet.txt
    networks:
      spnet:

  spark-master:
    image: biggis/spark:2.1.0
    hostname: spark-master
    ports:
      #- "4040:4040"
      - "6066:6066"
      - "7077:7077"
      - "8082:8080"
    command: start.sh master
    environment:
      USER_ID: ${USER_ID-1000}
      USER_NAME: spark
      TIMEZONE: Europe/Berlin
      HADOOP_MASTER_ADDRESS: hdfs-name
    networks:
      spnet:

  spark-worker:
    image: biggis/spark:2.1.0
    hostname: spark-worker
    depends_on:
      - spark-master
    command: start.sh worker
    ports:
      - "8081:8081"
    environment:
      USER_ID: ${USER_ID-1000}
      USER_NAME: spark
      TIMEZONE: Europe/Berlin
      SPARK_MASTER_ADDRESS: spark-master
      HADOOP_MASTER_ADDRESS: hdfs-name
    networks:
      spnet:

networks:
  spnet:
    driver: bridge
    ipam:
      config:
        - subnet: 172.30.0.0/16
          gateway: 172.30.0.1
